{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. PyTorch Transfer Learning\n",
    "\n",
    "> **Note:** This notebook uses `torchvision`'s new [multi-weight support API (available in `torchvision` v0.13+)](https://pytorch.org/blog/introducing-torchvision-new-multi-weight-support-api/).\n",
    "\n",
    "We've built a few models by hand so far.\n",
    "\n",
    "But their performance has been poor.\n",
    "\n",
    "You might be thinking, **is there a well-performing model that already exists for our problem?**\n",
    "\n",
    "And in the world of deep learning, the answer is often *yes*.\n",
    "\n",
    "We'll see how by using a powerful technique called [**transfer learning**](https://developers.google.com/machine-learning/glossary#transfer-learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is transfer learning?\n",
    "\n",
    "**Transfer learning** allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
    "\n",
    "For example, we can take the patterns a computer vision model has learned from datasets such as [ImageNet](https://www.image-net.org/) (millions of images of different objects) and use them to power our FoodVision Mini model.\n",
    "\n",
    "Or we could take the patterns from a [language model](https://developers.google.com/machine-learning/glossary#masked-language-model) (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use transfer learning?\n",
    "\n",
    "There are two main benefits to using transfer learning:\n",
    "\n",
    "1. Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\n",
    "2. Can leverage a working model which has **already learned** patterns on similar data to our own. This often results in achieving **great results with less custom data**.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-for-foodvision-mini%20.png\" alt=\"transfer learning applied to FoodVision Mini\" width=900/>\n",
    "\n",
    "*We'll be putting these to the test for our FoodVision Mini problem, we'll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.*\n",
    "\n",
    "Both research and practice support the use of transfer learning too.\n",
    "\n",
    "A finding from a recent machine learning research paper recommended practioner's use transfer learning wherever possible.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-how-to-train-your-vit-section-6-transfer-learning-highlight.png\" width=900 alt=\"how to train your vision transformer paper section 6, advising to use transfer learning if you can\"/>\n",
    "\n",
    "*A study into the effects of whether training from scratch or using transfer learning was better from a practioner's point of view, found transfer learning to be far more beneficial in terms of cost and time. **Source:** [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270) paper section 6 (conclusion).*\n",
    "\n",
    "And Jeremy Howard (founder of [fastai](https://www.fast.ai/)) is a big proponent of transfer learning.\n",
    "\n",
    "> The things that really make a difference (transfer learning), if we can do better at transfer learning, it’s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. — [Jeremy Howard on the Lex Fridman Podcast](https://youtu.be/Bi7f1JSSlh8?t=72)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "0.18.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # want 1.12+\n",
    "print(torchvision.__version__) # want 0.13+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we have the prefect version, so can use the pytorch transfer learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchWithCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
